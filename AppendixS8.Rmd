---
title: "Appendix S8: Parameter sensitivity analysis further defined"
output:
  pdf_document: default
  html_document: default
---

```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
rm(list=ls())
```

We may learn information from performing parameter sensitivity analysis with survival as an output rather than ER. This can tell us more about how a population is ecologically adapted to survive outside of ER. For example, performing parameter sensitivity analysis on the plague system model reveals that population survival is positively correlated with parameters $B$, $\mu_{\gamma}$, $K$, $\lambda$, $r$, and $\sigma$, and negatively correlated with $\delta$ and $\phi$. We expect, given the potential for ER in the main text, that host resistance rate and genetic variance ($\mu_{\gamma}$ and $\sigma$, respectively) would contribute to population survival. However, comparing ground squirrel and prairie dog parameters (see Table S1), we can see that ground squirrels are ecologically well-adapted to survive plague epizootics in a number of ways, compared to prairie dogs, including reduced reservoir transmission ($B$), slower transition from exposed to infectious ($\delta$), and slower transition from resistant to susceptible ($\phi$).\\

```{r PSA, warning=FALSE, message=FALSE}
setwd("C:/Users/bengo/Documents/ER/RevisionAnalyses")
library(fitur)
library(sensitivity)

SAmat.ind.P <- rbind(t(read.csv("SA_plagueREDO2.csv"))[-1,],t(read.csv("SA_plagueREDO2b.csv"))[-1,],t(read.csv("SA_plagueREDO2c.csv"))[-1,])
colnames(SAmat.ind.P) <- c("N.end","N.min","evolve","survive","g.start")
SAmat.ind.P.0 <- rbind(t(read.csv("SA_plague_0REDO2.csv"))[-1,],t(read.csv("SA_plague_0REDO2b.csv"))[-1,],t(read.csv("SA_plague_0REDO2c.csv"))[-1,])
colnames(SAmat.ind.P.0) <- c("N.end","N.min","evolve","survive","g.start")
SAmat.ind.SIR <- read.csv("SA_SIRSMlgREDO2.csv")
SAmat.ind.SIR <- t(SAmat.ind.SIR[,-1])
colnames(SAmat.ind.SIR) <- c("N.end","N.min","evolve","survive","g.start")
SAmat.ind.SIR.0 <- read.csv("SA_SIRSMlg_0REDO2.csv")
SAmat.ind.SIR.0 <- t(SAmat.ind.SIR.0[,-1])
colnames(SAmat.ind.SIR.0) <- c("N.end","N.min","evolve","survive","g.start")
LHS.PL.o <- rbind(read.csv("SA_LHS_plagueREDO2.csv")[,-1],read.csv("SA_LHS_plagueREDO2b.csv")[,-1],read.csv("SA_LHS_plagueREDO2c.csv")[,-1])
LHS.SIR.o <- read.csv("SA_LHS_SIRREDO2.csv")[,-1]

names(LHS.PL.o)[names(LHS.PL.o)=="period"] <- "delta"
names(LHS.SIR.o)[names(LHS.SIR.o)=="period"] <- "delta"

sims <- 200
n.PL <- length(SAmat.ind.P[,1])/sims
n.SIR <- length(SAmat.ind.SIR[,1])/sims


#Aggregate values
P.survive.PL <- rep(NA,n.PL)
N.min.PL <- rep(NA,n.PL)
P.survive.SIR <- rep(NA,n.SIR)
N.min.SIR <- rep(NA,n.SIR)
g.PL <- rep(NA,n.PL)
g.SIR <- rep(NA,n.SIR)
for(i in 1:n.PL){
  N.min.PL[i] <- median(SAmat.ind.P[((i-1)*sims+1):(i*sims),2])
  P.survive.PL[i] <- mean(SAmat.ind.P[((i-1)*sims+1):(i*sims),4])
  # N.min.SIR[i] <- median(SAmat.ind.SIR[((i-1)*sims+1):(i*sims),2])
  # P.survive.SIR[i] <- mean(SAmat.ind.SIR[((i-1)*sims+1):(i*sims),4])
  g.PL[i] <- mean(SAmat.ind.P.0[((i-1)*sims+1):(i*sims),5])
  # g.SIR[i] <- mean(SAmat.ind.SIR.0[((i-1)*sims+1):(i*sims),5])
}

for(i in 1:n.SIR){
  # N.min.PL[i] <- median(SAmat.ind.P[((i-1)*sims+1):(i*sims),2])
  # P.survive.PL[i] <- mean(SAmat.ind.P[((i-1)*sims+1):(i*sims),4])
  N.min.SIR[i] <- median(SAmat.ind.SIR[((i-1)*sims+1):(i*sims),2])
  P.survive.SIR[i] <- mean(SAmat.ind.SIR[((i-1)*sims+1):(i*sims),4])
  # g.PL[i] <- mean(SAmat.ind.P.0[((i-1)*sims+1):(i*sims),5])
  g.SIR[i] <- mean(SAmat.ind.SIR.0[((i-1)*sims+1):(i*sims),5])
}

P.survive.PL.0 <- rep(NA,n.PL)
N.min.PL.0 <- rep(NA,n.PL)
P.survive.SIR.0 <- rep(NA,n.SIR)
N.min.SIR.0 <- rep(NA,n.SIR)
for(i in 1:n.PL){
  N.min.PL.0[i] <- median(SAmat.ind.P.0[((i-1)*sims+1):(i*sims),2])
  P.survive.PL.0[i] <- mean(SAmat.ind.P.0[((i-1)*sims+1):(i*sims),4])
  # N.min.SIR.0[i] <- median(SAmat.ind.SIR.0[((i-1)*sims+1):(i*sims),2])
  # P.survive.SIR.0[i] <- mean(SAmat.ind.SIR.0[((i-1)*sims+1):(i*sims),4])
}
for(i in 1:n.SIR){
  # N.min.PL.0[i] <- median(SAmat.ind.P.0[((i-1)*sims+1):(i*sims),2])
  # P.survive.PL.0[i] <- mean(SAmat.ind.P.0[((i-1)*sims+1):(i*sims),4])
  N.min.SIR.0[i] <- median(SAmat.ind.SIR.0[((i-1)*sims+1):(i*sims),2])
  P.survive.SIR.0[i] <- mean(SAmat.ind.SIR.0[((i-1)*sims+1):(i*sims),4])
}


#determine Evolutionary Rescue for each parameter set
ER.PL <- P.survive.PL - P.survive.PL.0
ER.SIR <- P.survive.SIR - P.survive.SIR.0

param.PL <- 13
param.SIR <- 11

#sensitivity analysis SIRSMlg
Y.SIR <- pcc(LHS.SIR.o,P.survive.SIR,rank=TRUE)
T.SIR <- Y.SIR$PRCC*sqrt((n.SIR-2-param.SIR)/(1-Y.SIR$PRCC^2))
CI.SIR <- qt(c(0.025,0.975),n.SIR-2-param.SIR)
# T.SIR

#sensitivity analysis plague
Y.PL <- pcc(LHS.PL.o,P.survive.PL,rank=TRUE)
T.PL <- Y.PL$PRCC*sqrt((n.PL-2-param.PL)/(1-Y.PL$PRCC^2))
CI.PL <- qt(c(0.025,0.975),n.PL-2-param.PL)
# T.PL


#Make a bar chart comparison

library(tidyverse)
library(reshape2)
library(latex2exp)


T.dat <- as.data.frame(rbind(T.PL))
T.dat <- as.data.frame(cbind(T.dat,c(rep("Plague",13)),rep(rownames(T.PL),1)))
colnames(T.dat) <- c("value","model","parameter")



ggplot(T.dat) + theme_classic() + xlab("") + ylab("Sensitivity") +
  theme(text = element_text(size=16)) +
  geom_bar(aes(parameter,value,),fill="orange",stat="identity",position=position_dodge()) +
  scale_x_discrete(labels=c(expression(alpha),"B",expression(beta[c]),expression(beta[r]),expression(mu[gamma]),"K",expression(lambda),expression(mu),expression(delta),expression(phi),"r",expression(rho),expression(sigma))) +
  geom_hline(yintercept=CI.SIR,linetype=2)



```
\textbf{Figure FS11: }Parameter sensitivity analysis on the plague system model with survival as the output.\\

\vspace{5mm}

We know from our analysis of resistance rate and variance in the main text that there can be non-monotonic relationships between parameters and ER. To scan for this potential across our parameter sensitivity analysis, we systematically split parameter sets between low and high groups, one parameter at a time, and look for a reversal in relationship between the parameter and ER between the two groups. To automate this process, we perform the low/high split at all possible splitting points, and choose the splitting point that creates the greatest net relative sensitivity, i.e. maximizes $\left(\frac{\text{parameter sensitivity}_{\text{low}}}{\text{confidence interval}_{\text{low}}} + \frac{\text{parameter sensitivity}_{\text{high}}}{\text{confidence interval}_{\text{high}}}\right)$. We display the output below; for each parameter there are two figures of output to help visualize results. The first displays sensitivity results for the three parameter sets: the full set as seen in Figure 7 ("All"), portion of the given parameter set lower than the splitting point ("Low"), and the portion of the given parameter set higher than the splitting point ("High"). Closed dots are sensitivity values and open dots are 95\% confidence intervals. The second figure shows the parameter on the x axis with the ER value on the y axis, and overlays fitted linear regression models for the data as a whole (blue lines) and for separate low and high split parameter sets (red lines). Each point on these plots represents average survival over at least 200 runs of the given parameterization. Thusly, we can get an easy rough visulation of where non-monotonic effects might occur.\\

We perform this for the plague system model:\\

```{r Plague split, warning=FALSE, message=FALSE}
######PRCC Split X


#function to find the splitting point with maximum significance, starting with plague model

for(i in 1: param.PL){

T.low <- NA
CI.low <- NA
T.high <- NA
CI.high <- NA

LHS.sort <- LHS.PL.o[order(LHS.PL.o[,i]),]
ER.sort <- ER.PL[order(LHS.PL.o[,i])]
  
  #go through values one by one to split and find maximum effect
for(j in (3+2+param.PL):(n.PL-2-param.PL-3)){
    LHS.low <- LHS.sort[1:j,]
    ER.low <- ER.sort[1:j]
    LHS.high <- LHS.sort[(j+1):n.PL,]
    ER.high <- ER.sort[(j+1):n.PL]
    n.low <- length(ER.low)
    n.high <- length(ER.high)
    
    Y.low.temp <- pcc(LHS.low,ER.low,rank=TRUE)
    T.low.temp <- Y.low.temp$PRCC*sqrt((n.low-2-param.PL)/(1-Y.low.temp$PRCC^2))
    CI.low.temp <- qt(c(0.025,0.975),n.low-2-param.PL)
    
    Y.high.temp <- pcc(LHS.high,ER.high,rank=TRUE)
    T.high.temp <- Y.high.temp$PRCC*sqrt((n.high-2-param.PL)/(1-Y.high.temp$PRCC^2))
    CI.high.temp <- qt(c(0.025,0.975),n.high-2-param.PL)
    
    T.low <- cbind(T.low,T.low.temp)
    CI.low <- rbind(CI.low,CI.low.temp)
    
    T.high <- cbind(T.high,T.high.temp)
    CI.high <- rbind(CI.high,CI.high.temp)
}

T.low <- as.data.frame(t(T.low)[-1,])
T.high <- as.data.frame(t(T.high)[-1,])

CI.low <- as.data.frame(CI.low)[-1,]
CI.high <- as.data.frame(CI.high)[-1,]

#Calculate split with highest combined magnitude of effect
magnitude <- abs(T.low[,1]/CI.low[,2])+abs(T.high[,1]/CI.high[,2])
#Find the index of the corrseponding split IN THE LHS MATRIX
index.split <- which(magnitude==max(na.omit(magnitude)))

df.p1 <- as.data.frame(cbind(c(T.dat[i,1],T.low[index.split,i],T.high[index.split,i]),
               c(CI.PL[1],CI.low[index.split,1],CI.high[index.split,1]),
               c(CI.PL[2],CI.low[index.split,2],CI.high[index.split,2]),
               c("All","Low","High"),
               rep(colnames(LHS.sort)[i],3)))
colnames(df.p1) <- c("Value","CI.lower","CI.upper","Grouping","Parameter")

df.p2 <- as.data.frame(cbind(LHS.sort[,i],ER.sort,colnames(LHS.sort)[i]))
colnames(df.p2) <- c("Value","ER","Parameter")
par(mfrow=c(1,2))

df.p1$Grouping <- factor(df.p1$Grouping, levels=c("All","Low","High"))

p1 <- ggplot(df.p1) + xlab(colnames(LHS.sort)[i]) + ylab("Sensitivity") + theme_classic() +
  geom_hline(yintercept=0) +
  geom_point(aes(Grouping,as.numeric(as.character(Value)),group=1), size=5) +
  geom_point(aes(Grouping,as.numeric(as.character(CI.lower)),group=1), size=5, pch=1) +
  geom_point(aes(Grouping,as.numeric(as.character(CI.upper)),group=1), size=5, pch=1)

p2 <- ggplot(df.p2,aes(as.numeric(as.character(Value)),as.numeric(as.character(ER)))) + 
  xlab(colnames(LHS.sort)[i]) + ylab("ER") + theme_classic() +
  geom_point() +
  stat_smooth(method="lm") +
  stat_smooth(data=df.p2[1:(index.split + 2 + param.PL +2),], method="lm", color="red") +
  stat_smooth(data=df.p2[(index.split + 2 + param.PL +2):n.PL,], method="lm", color="red")

print(p1)
print(p2)

}#end screening all parameters for plague model
```


And now for the selected reduced model, which tends to show non-monotonic trends more clearly due to its greater parameter space capacity for ER:\\

```{r reduced split, warning=FALSE, message=FALSE}
#Repeat for the reduced RAMP model


for(i in 1: param.SIR){
  
  T.low <- NA
  CI.low <- NA
  T.high <- NA
  CI.high <- NA
  
  LHS.sort <- LHS.SIR.o[order(LHS.SIR.o[,i]),]
  ER.sort <- ER.SIR[order(LHS.SIR.o[,i])]
  
  #go through values one by one to split and find maximum effect
  for(j in (5+2+param.SIR):(n.SIR-2-param.SIR-5)){
    LHS.low <- LHS.sort[1:j,]
    ER.low <- ER.sort[1:j]
    LHS.high <- LHS.sort[(j+1):n.SIR,]
    ER.high <- ER.sort[(j+1):n.SIR]
    n.low <- length(ER.low)
    n.high <- length(ER.high)
    
    Y.low.temp <- pcc(LHS.low,ER.low,rank=TRUE)
    T.low.temp <- Y.low.temp$PRCC*sqrt((n.low-2-param.SIR)/(1-Y.low.temp$PRCC^2))
    CI.low.temp <- qt(c(0.025,0.975),n.low-2-param.SIR)
    
    Y.high.temp <- pcc(LHS.high,ER.high,rank=TRUE)
    T.high.temp <- Y.high.temp$PRCC*sqrt((n.high-2-param.SIR)/(1-Y.high.temp$PRCC^2))
    CI.high.temp <- qt(c(0.025,0.975),n.high-2-param.SIR)
    
    T.low <- cbind(T.low,T.low.temp)
    CI.low <- rbind(CI.low,CI.low.temp)
    
    T.high <- cbind(T.high,T.high.temp)
    CI.high <- rbind(CI.high,CI.high.temp)
  }
  
  T.low <- as.data.frame(t(T.low)[-1,])
  T.high <- as.data.frame(t(T.high)[-1,])
  
  CI.low <- as.data.frame(CI.low)[-1,]
  CI.high <- as.data.frame(CI.high)[-1,]
  
  #Calculate split with highest combined magnitude of effect
  magnitude <- abs(T.low[,1]/CI.low[,2])+abs(T.high[,1]/CI.high[,2])
  #Find the index of the corrseponding split IN THE LHS MATRIX
  index.split <- which(magnitude==max(na.omit(magnitude)))
  
  df.p1 <- as.data.frame(cbind(c(T.dat[i+param.SIR,1],T.low[index.split,i],T.high[index.split,i]),
                               c(CI.SIR[1],CI.low[index.split,1],CI.high[index.split,1]),
                               c(CI.SIR[2],CI.low[index.split,2],CI.high[index.split,2]),
                               c("All","Low","High"),
                               rep(colnames(LHS.sort)[i],3)))
  colnames(df.p1) <- c("Value","CI.lower","CI.upper","Grouping","Parameter")
  
  df.p2 <- as.data.frame(cbind(LHS.sort[,i],ER.sort,colnames(LHS.sort)[i]))
  colnames(df.p2) <- c("Value","ER","Parameter")
  par(mfrow=c(1,2))
  
  df.p1$Grouping <- factor(df.p1$Grouping, levels=c("All","Low","High"))
  
  p1 <- ggplot(df.p1) + xlab(colnames(LHS.sort)[i]) + ylab("Sensitivity") + theme_classic() +
    geom_hline(yintercept=0) +
    geom_point(aes(Grouping,as.numeric(as.character(Value)),group=1), size=5) +
    geom_point(aes(Grouping,as.numeric(as.character(CI.lower)),group=1), size=5, pch=1) +
    geom_point(aes(Grouping,as.numeric(as.character(CI.upper)),group=1), size=5, pch=1)
  
  p2 <- ggplot(df.p2,aes(as.numeric(as.character(Value)),as.numeric(as.character(ER)))) + 
    xlab(colnames(LHS.sort)[i]) + ylab("ER") + theme_classic() +
    geom_point() +
    stat_smooth(method="lm") +
    stat_smooth(data=df.p2[1:index.split + 2 + param.SIR,], method="lm", color="red") +
    stat_smooth(data=df.p2[(index.split + 2 + param.SIR):n.SIR,], method="lm", color="red")
  
  print(p1)
  print(p2)
  
}#end screening all parameters for SIR model
```



When the above analysis gives indication of potential nonmonotonic effects (e.g. $r$ in the reduced model), we can investigate further to select a splitting point by hand to fine tune results.